---
layout: post
title:  "EbbRT MRI Fetal Reconstruction - 2017 Update"
date:   2018-01-05
category: research
---

* [Overview](#overview)
* [Background](#background)
* [Results](#results)
  * [Preliminary Results](#preliminary-results)
  * [Latest Results](#latest-results)
* [Next Steps](#future-work)
  * [Scalability Limit](#scalability-limit)
  * [Configuration Study](#configuration-study)
  * [Front-end Bottlenecks](#front-end-bottlenecks)
* [Conclusion](#conclusion)
* [Sources](#sources)


# Overview

This report presents the latest on the EbbRT MRI Fetal Reconstruction project
done by the SESA research group at Boston University. In particular, this report
contains the latest performance results of the `EbbRT-fetalrecon` application,
along with observations and recommended next steps for the project.  

The research investigates software elasticity applied to real-world
applications, that is, applications made able to dynamically
grown-and-shirk resources sizes on-demand. The approach is to improve the runtime performance of
compute-intensive algorithms by by distributing the computation across
dynamically-allocated "cloud" resources. The `EbbRT` software demonstrates
that such improvements are achievable using simple software engineering
techniques and off-the-shelf services.  

<!--
Through EbbRT we hope to demonstrate
the following: 

+ prototype application that represents a class of
  _scale-out_ scientific applications able to benefit from dynamic resource elasticity.
+ Investigate the software requirements and designs patterns that necessitates
  enabling the efficient use of elastic resources. 
-->

# Background

<!--
An active field of operating system / cloud computing research is that of
**software elasticity**, that is, software able to dynamically
grown and shirk its set of allocated resources on-demand.  Exploring software
elasticity _at scale_ is the primary research goal of the _SESA research
group_ at BU, and the general topic of this report. 
-->

This project is based on the [MRI Fetal
Reconstruction](https://github.com/bkainz/fetalReconstruction) application,
originally developed by [BioMedIA](https://biomedia.doc.ic.ac.uk/), used for generating "superresolution"
model reconstructions from a series of motion-corrupted MRI images. The
command-line utility takes as input a source set of images and the parameters 
used to configure the reconstruction job, and produces a single output image
file.  

We focus on specializing the algorithm responsible for transforming the input
images into the reconstructed output file. This algorithm consists of multiple
rounds of compute-intensive phases.  Phases that highly parallel are
distributed across a pool of _worker_ threads, while sequential phases are
handled by the main thread of the algorithm.

The original _MRI Fetal Reconstruction_ application distributed work to a
local GPU device (using the _NVIDIA CUDA Toolkit_) or across the CPUs of a
single machine.  The [EbbRT framework](https://github.com/SESA/EbbRT) enables
the application to distributed the parallel works across the threads of a
dedicated set of virtual machines, which are dynamically allocated for each
reconstruction job.  This framework provides facilitates for building
application source code into specialized _back-end_ kernels that are bootable
inside virtual machine instances. Additional facilities provided enable a
_front-end_ (Linux) application to create and communicate with these back-end
instances.  The resulting `EbbRT-fetalRecon` application consists of
_front-end_ Linux application and multiple and _back-end_ nodes, which
together coordinating to complete the reconstruction job.


# Results

### Preliminary Results

Previously, the work of this projected focus on the implementation of the
distributed algorithm and the verification of the result compared to the
original. Preliminary results[^c3] (shown in **Table 1**) where collected
using the "large" dataset that accompanies the MRI Fetal Reconstruction
application source, and an "ideal" default configuration for the
reconstruction job[^c1]. 
```
┌────────────────────┬─────────────────────┬─────────────────────┬──────────────────────┐
│                    │ Original App        │ EbbRT-fetalRecon    │ Improvement          │
├────────────────────┼─────────────────────┼─────────────────────┼──────────────────────┤
│ Compute Time (s)   │ 528, 371, 291       │ 431, 247, 163       │ +18%, +33%, +43%     │
├────────────────────┼─────────────────────┼─────────────────────┼──────────────────────┤
│ Total Runtime (s)  │ 548, 390, 311       │ 674, 504, 434       │ -22%, -29%, -39%     │
└────────────────────┴─────────────────────┴─────────────────────┴──────────────────────┘
```
> **Table 1:**  Results (gathered 5/17) of the reconstruction algorithm. 8
> iterations, large dataset with **5,10,15** worker threads. On
> EbbRT-fetalRecon, 5 worker threads per back-end[^c3].


The measurements in **Table 1** compare the runtime of `EbbRT-fetalRecon` with a
multi-threaded CPU deployment of the unmodified MRI Fetal Reconstruction application. These results show
a 43% improvement of `EbbRT-fetalRecon` in the runtime of the  
compute phases of the algorithm.  However, the overall runtime of the
application was 39% slower on `EbbRT-fetalRecon`, as the progress of the
computation was initially impeded by the allocation and communication costs of the back-ends. 

## Latest Results

This past semester work was done to assess the performance results and make
improvements. This assessment determined that the majority of the
`EbbRT-fetalRecon` runtime was not spent on the underlying algorithm or
workload, but was instead was spent sending/receiving data, or blocked waiting
for back-ends nodes to finish.  This loss in performance was ultimate
associated to a few design flaws in the implementation of `EbbRT-fetalRecon`
front-end, which were addressed and measured (shown in **Table 2**). Once the
design flaws were fixed, overall runtime of `EbbRT-fetalRecon` improved by 58%
(434s down to 185s), which accounts for a 40% improvement in overall runtime
compared to the original application. 

```
┌────────────────────┬──────────────┬──────────────────┬─────────────┐
│                    │ Original App │ EbbRT-fetalRecon │ Improvement │
├────────────────────┼──────────────┼──────────────────┼─────────────┤
│ Compute Time (s)   │ 291          │ 170.12           │ +41%        │
├────────────────────┼──────────────┼──────────────────┼─────────────┤
│ Total Time (s)     │ 311          │ 184.94           │ +40%        │
└────────────────────┴──────────────┴──────────────────┴─────────────┘
```
> **Table 2:**  Updated results of the reconstruction algorithm (8 iterations, large dataset, 3 EbbRT back-ends, 15 back-end worker threads)

# Next Steps 

This project has established the correctness of the distributed
algorithm and shown an initial performance gain (of about 40%) over an
unmodified version. The next steps, propose in the report, to establish the degree to which the
reconstruction algorithm is able to improve with scale, and gather a
near-optimal speedup results for the available dataset and experimental
environment.  Towards fulfilling this objective, we propose three approaches, each
of which is discuss below:  

1. Examination of the underlying algorithm to define a theoretical
   scalability limit in accordance to Amdahl's law of scalability.
1. Determine an ideal arrangement and configuration of the available distributed resources for a
   particular job configuration.
1. Alleviate compute bottlenecks by introducing fine-grain parallelism on the front-end.


## Scalability Limit

Analyzing the reconstruction algorithm in accordance with Amdahl's law of
scalability will result in a theoretically limit of achievable speedup via
increased scale.  From this limit we can infer
the total amount of worker threads necessary to produce the ideal runtime
performance.

![](https://image.ibb.co/mjUZQG/fetalrecon_notes_001_copy.png)

Key to defining a scalability limit is knowing the ratio between parallel and
sequential phases of the algorithm.  The above figure shows a _hypothetical_ breakdown of runtime performance
between the parallel ("back-end") and sequential ("front-end",
"initial") phases of the algorithm as well as relative communication overheads ("IO"). The x-axis
represents the reconstruction _iterations_, a runtime parameter that
increases the total amount of compute and IO necessary to complete the job 


```
┌──────────────────┬─────────┬─────────────┬────────────┬──────────────┬────────┬─────────┐
│                  │ Initial │ F/e Work    │ [F/e wait] │ B/e Work     │ [Sum]  │ Runtime │
├──────────────────┼─────────┼─────────────┼────────────┼──────────────┼────────┼─────────┤
│ EbbRT-fetalRecon │ 14.82   │ 53.56       │ 95.4       │ 106.6 (max)  │ 174.98 │ 184.94  │
└──────────────────┴─────────┴─────────────┴────────────┴──────────────┴────────┴─────────┘
```
> **Table 3:**  Breakdown of time (s) spent in compute phases of the algorithm  
(same configuration as Table 2)

An internal breakdown of the latest `EbbRT-fetalRecon` performance
results (shown in **Table 3**)  show that roughly 30% of the total time is spent doing
sequential processing on the front-end ("`Initial`" and "`F/e Compute`"),
suggesting that the remaining computation time is able of being improved
with adding additional resources.  To this point, we stress the implementation across various scales shown in
**Table 4** below.


## Configuration Study

EbbRT enables applications control both the amount of back-end
nodes to allocate and individual resource configuration of those nodes (e.g., CPUs, RAM).
A known scalability limit can be used to determine an ideal amount of parallel
threads to dedicate to a particular task, but does not suggest how these threads
should be arranged.  For example, for a fixed amount of workers, is
it is better to allocate many back-ends VMs with fewer VCPUs, or
fewer back-end VMs with more CPUs.

To answer this question, we can measure the baseline behaviour of the
distributed algorithm's across various input configurations. By carefully
measuring the communication overheads (data sent/received) and computational
requirements (cycles) for each phases of the algorithm,  we can determine how
the underlying workload is affected by the various input configurations. This,
together with the knowledge of the expected performance of increase nodes and
core counts, can be used to programmatically determine an ideal configuration
for the back-end set.

![](https://image.ibb.co/d1kVzb/fetalrecon_notes_001.png)

**The above figure** surmises the scalability advantage that the application
is able to achieve by partitioning its workload across **(i)** multiple
back-end threads and **(ii)** multiple back-end nodes.  As the image predicts
(and the preliminary results supports), adding additional back-ends will see
an improvement in total runtime, but the advantage will diminish as
front-end/IO processing becomes the overall bottleneck. 


```
┌─────────────────────┬──────────┬───────┬──────────┬──────────┬──────────┬───────────┐
│                     │ B/e CPUs │ Alloc │ F/e work │ F/e wait │ B/e work │ Total(s)  │
├─────────────────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────┤
│ EbbRT 3-node,7cpu   │ 21       │ 9.2   │ 67.7     │ 95.4     │ 82.0     │ 164.1     │
├─────────────────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────┤
│ EbbRT 6-node,5cpu   │ 30       │ 9.3   │ 68.74    │ 78.2     │ 57.4     │ 147.0     │
├─────────────────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────┤
│ EbbRT 3-node,12cpu  │ 36       │ 9.4   │ 67.9     │ 64.7     │ 55.2     │ 133.0     │
├─────────────────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────┤
│ EbbRT 6-node,7cpu   │ 42       │ 9.4   │ 68.7     │ 74.4     │ 54.2     │ 144.0     │
└─────────────────────┴──────────┴───────┴──────────┴──────────┴──────────┴───────────┘
```

> **Table 4:**  Breakdown of times in the compute phases of the algorithm  
(8 iterations, large dataset)

The results of **Table 3** and **Table 4** present a
limited number of configurations of the `EbbRT` back-end nodes, chosen intentionally for the
underlying hardware environment (four physical servers / 12 CPU per machines).  
In each experiment, the front-end application was dedicate its own physical machine, and the 
back-ends VMs were distributed evenly across the remaining three machines
(which have a total of 36 CPUs). 

The best performance results shown where gather with three back-end VM nodes
each having 12 VCPUs (mappable 1:1 to the underlying machines). This suggests
that the configurations reaches a point where the improvement of added
parallelism in an additional back-end VM does account for the added
communication and allocation overheads. In other words, fewer "beefier"
back-ends nodes are preferred. A better understanding of the
compute-vs-communication requirements of the algorithm will allow use to know
if this, or similar, findings are generally applicable for all possible
configuration of algorithm.


## Front-end Bottlenecks 

An breakdown of the application runtime (shown that **Table 3**) shows that
_**over one minute**_ is spent doing sequential processing on the front-end.
This suggests that front-end processing will become the bottleneck of the
application as parallelism is added to the back-ends. It maybe possible to
alleviate this this bottleneck by distributing the processing across local
threads.  Futhermore, we maybe able to minimise the time the front-end is
spend blocked by processing back-end result data as they arrive via streams.
To do either of these optimisations first requires a better understanding of
the work the algorithm does on the front-end, and any ordering requirements of
these operations.

## Conclusion

By continuing work on this project we can define an ideal performance target
for the `EbbRT-fetalRecon` application, which, in comparison to the original
CPU implementation, can determine just how much this algorithm can improved
using dynamic allocations and `EbbRT`. Furthermore, the ability to configure
allocated resources at runtime, combined with the variadic nature of the
reconstruction algorithm, presents an opportunity to prototype an application
that can programmatically select the amount and arrangement of back-ends
resource at runtimes, which presents and advanced use of  "elasticity" in
application software.

---

### Sources:

[^c1]: Kainz et al; **Fast Volume Reconstruction from Motion Corrupted Stacks of 2D Slices**; _IEEE Transactions on Medical Imaging_; 2015  

[^c2]: Schatzberg et al; **EbbRT: A Framework for Building Per-Application Operating Systems**; _12th USENIX Symposium on Operating Systems Design and Implementation_; 2016

[^c3]: Alibrandi, Arguello et al; **An EbbRT distributed application for MRI Fetal Reconstruction**; Internal Technical Report; 2017
