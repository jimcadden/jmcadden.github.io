---
layout: post
title:  HotCloud'18 Paper Outline 
date:   2018-02-05
update:   2018-02-07
categories: "research cloud"
description: Early Outline for HotCloud'18 Paper 
permalink: hotcloud18 
---

# Summary
This evolving document contains an outline/summary for a HotCloud 2018
submission.

This paper argues that cloud applications should utilize an _elastic
library operating system_ layer to better exploit the scale and elasticity of
the underlying cloud utility.  The motivation for a radical system-layer
redesign is driven by the widespread adoption of "reactive" application
models, particularly where applied to highly-parallel (HPC-like) workloads,
and the new trends of bare-metal cloud platforms, and "composable" datacenter
architectures.         

We envision a future cloud platform where distributed application grown in
size by speaking directly with the underlying cloud utility layer, in as
naturally and efficiently as an applications today speaks to the OS to spawn
additional threads, processes or containers.

In an environment where additional hardware resources (i.e., compute nodes)
can be provisioned quickly and allocated directly to clients, it is the
client's _system layers_ --the operating systems and distributed middleware--
the becomes a bottleneck for dynamic growth and software agility. Simply put, the
overhead to startup a full-featured operating system, and the requisite set of
network-configured middleware service-points, is simply too large to do in
response to fine-grain/rapid fluctuations applications size. 

Beyond shortening the startup time, a library operating layer can include
"baked-in", the distributed system components, programming models and first-order "elasticity"
primitives that will be necessary for the application to managing its dynamic
set of distributed components. 

<!--
Furthermore, finely-tuned elastic PaaS-like platforms can dynamically consume
raw resources as the amount of external customer demands fluctuates. To this
latter examples, we provide a prototype of a distributed javascript runtime
deployed on an elastic-aware libraryOS, and deployed dynamically on a
bare-metal cloud resources.  
-->
## Outline

1. Introduction / Problem / Motivations
    * Demand for fine-grain elasticity
1. Supporting Trends/Observations
    * Software Treads 
      * Reactive application model 
      * Serverless / PaaS
      * HPC on Cloud
    * Hardware Trends
      * Bare-metal cloud / HaaS
      * "Composable" infrastructure
        * Disaggregate datacenters
      * Future hardware design
        * system on-a-chip, etc
1. Elastic Application OS 
    * Approach
      * LibraryOS
      * Built-in distributed system components
      * Elastic-aware primatives
    * Benefits
      * Small binary Size
        * Fast boot
        * Small attack surface
      * Programability 
        * "cloud native" unit of execution
        * Unfied distributed components/interface
        * Crash-only (disposible) failure design
      * Low-level hardware access 
        * performance (kernel bypass)
        * cost vs efficency (cloud utility)
1. Related Work 
    * Unikernels 
    * PaaS / Heroku
1. Challenges


---

> **OLD/STALE CONTENT BELOW THIS POINT**


# Summary


This vision paper argues that an "elastic" application runtime, a distributed
set of LibraryOS instances, is necessary for enabling fine-grain elasticity at
scale.  

This paper argues for a radical redesign of the cloud software stack, from
multiple tiers of legacy interfaces and middleware, to a "flattened"
distributed library operating system dedicated to an single application or
language runtime.   

The motivation for this shift is finally enable cloud applications the ability
to fully exploit the  immense scale and fine-grain elasticity provided by the
underlying cloud utility.  This becomes necessary as the _instantaneous
concurrency_ demands of cloud applications exceed what is readily available in
pre-allocated resource.  As fast, fine-grain resource demands of the
application push the underlying "system" to be expended, the scale and
performance of the application is inhibited by the _inflexible_ properties of
the software stack which the application depends on. 

**Hypothesis:** By removing and condensing the existing "dependency layers" of
the cloud software stack, we enable a distributed application runtime that is
agile enough to grow to consume one to many dedicated CPUs, and back again,
at the pace of the underlying resource utility provider.  


# Outline

1. Introduction 
    * Problem: elastic bottleneck
1. Motivations
    * Demand for fine-grain elasticity
      * Definition
      * Examples / Use Cases
1. Supporting Trends & Incentives 
    * Reactive Programming
      * Serverless
    * Paas / Faas Platforms
      * Serverless
      * Heroku
    * "Composable Infrastructure"
      * HaaS
      * Disaggregate datacenters
      * Dark silicon
1. Elastic Application Runtime 
    * Approach
      * LibraryOS
      * Built-in distributed system components
      * Elastic-aware primatives
    * Benefits
      * Binary Size
        * Fast boot
        * Small attack surface
      * Simplicity
        * New unit of execution
        * Unfied "distributed OS" interfaces
        * Crash-only failure design
      * Low-level access 
        * High performance (kernel bypass)
1. Related Work 
    * Unikernels 
    * PaaS
1. Challenges

<!-- # Scratch 

These layers include cloud
infrastructure "services" (PaaS, FaaS, many SaaS), a myriad of
application "micro-services", language runtimes, containers, traditional
operating systems, and virtualisation.

Fine-grain elasticity is the dynamic (on-demand) allocation/deallocations that
is fast, one to many CPUs. Scale capable of consuming the resources of many
_physical_ machines.

### Titles
+ The final stretch: fine-grain elasticity at scale
+ The final stretch: fine-grain elasticity at massive scale
+ The final stretch: fine-grain elasticity at datacenter scale
+ The final stretch towards fine-grain elasticity at scale

### Data/Results

1. Node.js
  1. back-end boot/config times (~seconds)
  1. scale of concurrent back-ends 
  1. I/O throughput 

-->
