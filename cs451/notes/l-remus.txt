6.824 2015 Lecture 3: Primary/Backup Replication

Today
  Replication
  Remus case study
  Lab 2 introduction

Fault tolerance
  we'd like a service that continues despite failures!
  available: still useable despite [some class of] failures
  correct: act just like a single server to clients
  very hard!
  very useful!

Need a failure model: what will we try to cope with?
  Independent fail-stop computer failure
    Remus further assumes only one failure at a time
  Site-wide power failure (and eventual reboot)
  (Network partition)
  No bugs, no malice

Core idea: replication
  *Two* servers (or more)
  Each replica keeps state needed for the service
  If one replica fails, others can continue

Example: fault-tolerant MapReduce master
  lab 1 workers are already fault-tolerant, but not master
    master is a "single point of failure"
  can we have two masters, in case one fails?
  [diagram: M1, M2, workers]
  state:
    worker list
    which jobs done
    which workers idle
    TCP connection state
    program counter

Big Questions:
  What state to replicate?
  How does replica get state?
  When to cut over to backup?
  Are anomalies visible at cut-over?
  How to repair / re-integrate?

Two main approaches:
  State transfer
    "Primary" replica executes the service
    Primary sends [new] state to backups
  Replicated state machine
    All replicas execute all operations
    If same start state,
      same operations,
      same order,
      deterministic,
      then same end state

State transfer is simpler
  But state may be large, slow to transfer
  Remus uses state transfer

Replicated state machine can be more efficient
  If operations are small compared to data
  But complex, e.g. order on multi-core, determinism
  Labs use replicated state machines

Remus: High Availability via Asynchronous Virtual Machine Replication
NSDI 2008

Very ambitious system:
  Whole-system replication
  Completely transparent to applications and clients
  High availability for any existing software
  Would be magic if it worked well!
  Failure model:
    1. independent hardware faults
    2. site-wide power failure

Plan 1 (slow, broken):
  [diagram: app, O/S, Remus underneath]
  two machines, primary and backup; plus net and other machines
  primary runs o/s and application s/w, talks to clients, &c
  backup does *not* initially execute o/s, applications, &c
    it only executes some Remus code
  a few times per second:
    pause primary
    copy entire RAM, registers, disk to backup
    resume primary
  if primary fails:
    start backup executing!

Q: is Plan 1 correct?
   i.e. does it look just like a single reliable server?

Q: what will outside world see if primary fails and replica takes over?
   will backup have same state as last visible on primary?
   might a client request be lost? executed twice?

Q: is Plan 1 efficient?

Can we eliminate the fact that backup *state* trails the primary?
  Seems very hard!
  Primary would have to tell backup (and wait) on every instruction.

Can we *conceal* the fact that backup's state lags primary?
  Prevent outside world from *seeing* that backup is behind last primary state
    e.g. prevent primary sent RPC reply but backup state doesn't reflect that RPC
    e.g. MR Register RPC, which it would be bad for backup to forget
  Idea: primary "holds" output until backup state catches up to output point
    e.g. primary receives RPC request, processes it, creates reply packet,
    but Remus holds reply packet until backup has received corresponding state update

Remus epochs, checkpoints
  Clients:    C1
               req1                       reply1
  Primary:    ... E1 ... | pause |  E2  release   | pause |
                           ckpt        ok           ckpt
  Backup:      ... (E0) ... |  apply  | (E1)         | 
  1. Primary runs for a while in Epoch 1, holding E1's output
  2. Primary pauses
  3. Primary sends RAM+disk changes to backup (in background)
  4. Primary resumes execution in E2, holding E2's output
  5. Backup copies all to separate RAM, then ACKs
  6. Primary releases E1's output
  7. Backup applies E1's changes to RAM and disk

If primary fails, backup finishes applying last epoch's disk+RAM,
  then starts executing

Q: any externally visible anomalies?
   lost input/output?
   repeated output?

Q: what if primary receives+executes a request, crashes before checkpoint?
   backup won't have seen request!

Q: what if primary crashes after sending ckpt to backup,
   but before releasing output?

Q: what if client doesn't use TCP -- doesn't re-transmit?

Q: what if primary fails while sending state to backup?
   i.e. backup is mid-way through absorbing new state?

Q: are there situations in which Remus will incorrectly activate the backup?
   i.e. primary is actually alive
   network partition...

Q: when primary recovers, how does Remus restore replication?
   needed, since eventually active ex-backup will itself fail

Q: what if *both* fail, e.g. site-wide power failure?
   RAM content will be lost, but disks will probably survive
   after power is restored, reboot guest from one of the disks
     O/S and application recovery code will execute
   disk must be "crash-consistent"
     so probably not the backup disk if was in middle of installing checkpoint
   disk shouldn't reflect any held outputs (... why not?)
     so probably not the primary's disk if was executing
   I do not understand this part of the paper (Section 2.5)
     seems to be a window during which neither disk could be used if power failed
       primary writes its disk during epoch
       meanwhile backup applies last epoch's writes to its disk

Q: in what situations will Remus likely have good performance?

Q: in what situations will Remus likely have low performance?

Q: should epochs be short or long?

Remus Evaluation
  summary: 1/2 to 1/4 native speed
  checkpoints are big and take time to send
  output hold limits speed at which clients can interact

Why so slow?
  checkpoints are big and take time to generate and send
    100ms for SPECweb2005 -- because many pages written
  so inter-checkpoint intervals must be long
  so output must be held for quite a while
  so client interactions are slow
    only 10 RPCs per second per client

How could one get better performance for replication?
  big savings possible with application-specific schemes:
    just send state really needed by application, not all state
    send state in optimized format, not whole pages
    send operations if they are smaller than state
  likely *not* transaparent to applications
    and probably not to clients either

